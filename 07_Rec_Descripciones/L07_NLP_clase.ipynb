{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BCDfXwoiUp4O"
   },
   "source": [
    "<div >\n",
    "<img src = \"../Machine Learning Aplicado al Marketing-4875-x-834.jpg\" />\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gVk5YyKOUp4U"
   },
   "source": [
    "# Procesamiento del lenguaje natural"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ignaciomsarmiento/MALM/blob/main/07_Rec_Descripciones/L07_NLP_clase.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducción\n",
    "\n",
    "Los sistemas basados en contenido, a diferencia de los filtros colaborativos, no requieren datos relacionados a otros individuos u actividades pasadas. Por el contrario, estos brindan recomendaciones basadas en el perfil del usuario y los metadatos que se tiene sobre elementos particulares.\n",
    "\n",
    "Si bien en este tipo de sistemas se utilizan la interacción entre un usuario y un ítem particular (lo compró, calificó, etc.) nosotros nos centraremos en el uso de texto asociado a los ítems.\n",
    " \n",
    "Este *cuaderno* hace una breve introducción a herramientas de procesamiento de lenguaje. El objetivo del *cuaderno* es que usted aprenda el funcionamiento  de las mismas para poder llevar a cabo procesamientos básicos del lenguaje natural que vamos a necesitar para sistemas de recomendación basados en contenido."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gn0N1V-zUp4V"
   },
   "source": [
    "## Introducción"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sFUb0aDMUp4V"
   },
   "source": [
    "Para motivar el estudio de expresiones regulares veamos la siguiente conversación realizada en 1966 por un chatbot llamado ELIZA y un ser humano:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PUEBAGY4Up4V"
   },
   "source": [
    "*Usuario: Me siento triste.*\n",
    "\n",
    "*ELIZA: ¿Cree que venir acá puede ayudarle a no sentirse triste?*\n",
    "\n",
    "*Usuario: Necesito ayuda, de eso estoy seguro.*\n",
    "\n",
    "*ELIZA: ¿Qué significa para usted obtener ayudar?*\n",
    "\n",
    "*Usuario: A lo mejor podría aprender a llevárme mejor con mi madre.*\n",
    "\n",
    "*ELIZA: Cuénteme más sobre su madre.*\n",
    " \n",
    "*Usuario: Mi madre cuida de mí.*\n",
    "\n",
    "*ELIZA: ¿Quién más en su familia cuida de usted?*\n",
    "\n",
    "*Usuario: Mi padre.*\n",
    "\n",
    "*ELIZA: ¿Su padre?*\n",
    "\n",
    "*Usuario: De alguna manera usted se parece a mi padre.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AOcAwv93Up4W"
   },
   "source": [
    "El fragmento anterio fue extraido de Weizenbaum (1966) y muestra como ELIZA, uno de los primeros sistemas de procesamiento del lenguaje natural, imita respuestas de un psicoterapeuta. Es más, este sistema fue programado para para imitar las respuestas de un [psicoterapeuta Rogersiano](https://es.wikipedia.org/wiki/Psicoterapia_centrada_en_el_cliente). \n",
    "\n",
    "Este programa ELIZA es muy simple y exitoso a la vez. Simple porque está entrenado para identificar frases del estilo *\"Necesito X\"* y responder frases del estilo *\"¿Qué significa para usted obtener X?\"*. El éxito, por otro lado, consiste en que este no tiene que tener conocimientos previos sobre el significado de las palabras. Este tipo de diálogos permiten que quien hace de terapeuta (ELIZA en este caso) no tenga que saber nada sobre el mundo. \n",
    "\n",
    "De hecho, Weizenbaum (1966) reporta que muchas de las personas que interactuaron con este bot creyeron que ELIZA en verdad los estaba entendiendo, inclusive, luego de que se les explicara a los participantes cómo funcionaba ELIZA, muchos seguían pensando que este robot era útil para ayudarlos a lidiar con sus problemas.\n",
    "\n",
    "Hoy en día, los algoritmos de Procesamiento de Lenguaje Natural son mucho más complejos que ELIZA, sin embargo, la detección de patrones en el texto sigue siendo una base fundamental de este tipo de aplicaciones. En lo que resta del *cuaderno* exploraremos los conceptos básicos de cómo procesar lenguaje natural escrito, texto, para que sea interpretable por el computador, nos permita  extraer información relevante y hacer pre procesamiento necesario para que pueda ser el insumo de sistemas de recomendación."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nvOtUvflUp4X"
   },
   "source": [
    "## Procesamiento de texto\n",
    "\n",
    "Antes de poder utilizar el texto necesitamos transformarlo de forma tal que las maquinas puedan utilizarlo. Hay al menos 3 pasos previos:\n",
    "\n",
    "   1. **Limpieza de texto**, es un paso crucial en el procesamiento de lenguaje natural (NLP). Durante esta fase, se preparan los datos eliminando ruido y estructurando el texto para que los algoritmos puedan interpretarlo mejor. Es el proceso de transformar el texto bruto en una forma más estructurada y homogénea, eliminando información irrelevante o inconsistente. **Las expresiones regulares (regex)** juegan un papel importante al automatizar tareas específicas de limpieza. \n",
    "\n",
    "      1. **Eliminación de caracteres no deseados**:\n",
    "         - Ejemplo: Eliminación de puntuación, símbolos o caracteres especiales.\n",
    "         - Regex típica: `r\"[^\\w\\s]\"` para eliminar todo excepto letras, números y espacios.\n",
    "\n",
    "      2. **Normalización de texto**:\n",
    "         - Convertir texto a **minúsculas** para evitar duplicidades como \"Casa\" y \"casa\".\n",
    "         - Regex no necesaria, pero funciones como `.lower()` ayudan.\n",
    "\n",
    "      3. **Eliminación de números (si no son útiles)**:\n",
    "         - Ejemplo: Quitar cifras en contextos donde no aportan valor.\n",
    "         - Regex: `r\"\\d+\"` para eliminar dígitos.\n",
    "\n",
    "      4. **Eliminación de espacios adicionales**:\n",
    "         - Regex: `r\"\\s+\"` para reemplazar múltiples espacios por uno solo.\n",
    "\n",
    "      5. **Eliminación de URLs o direcciones web**:\n",
    "         - Regex: `r\"http\\S+|www\\.\\S+\"` para detectar URLs.\n",
    "\n",
    "      6. **Eliminación de menciones y hashtags (en texto social)**:\n",
    "         - Ejemplo: \"@usuario\" o \"#tema\".\n",
    "         - Regex: `r\"@\\w+|#\\w+\"`.\n",
    "\n",
    "      7. **Sustitución de contracciones o abreviaturas**:\n",
    "         - Ejemplo: \"I'm\" → \"I am\".\n",
    "         - No siempre requiere regex; puede hacerse con diccionarios de sustitución.\n",
    "\n",
    "      8. **Eliminación de palabras vacías (stopwords)**:\n",
    "         - Ejemplo: \"el\", \"y\", \"de\".\n",
    "         - En esta etapa pueden usarse listas predefinidas en lugar de regex.\n",
    "\n",
    "\n",
    "\n",
    "      Esta etapa: \n",
    "\n",
    "      - Reduce el ruido en el texto, haciendo que los modelos se centren en información relevante.\n",
    "      - Mejora la consistencia del texto, facilitando el análisis y la tokenización posterior.\n",
    "      - Automatiza tareas repetitivas mediante el uso de expresiones regulares.\n",
    "\n",
    "\n",
    "   2. **Tokenización.** La tokenización es el proceso de dividir un texto en unidades más pequeñas, llamadas *tokens*. \n",
    "   \n",
    "      - **Tipos de tokenización**:\n",
    "         - **Por palabras**: Divide el texto en palabras individuales. Ejemplo: \"Hola, mundo\" → [\"Hola\", \",\", \"mundo\"].\n",
    "         - **Por caracteres**: Cada carácter se convierte en un token. Ejemplo: \"Hola\" → [\"H\", \"o\", \"l\", \"a\"].\n",
    "         - **Por subpalabras**: Divide en unidades más pequeñas que las palabras completas, común en modelos de lenguaje modernos como BERT. Ejemplo: \"jugando\" → [\"jug\", \"ando\"].\n",
    "\n",
    "      - **Usos principales**:\n",
    "         - **Procesamiento de Lenguaje Natural (PLN)**: Permite a los modelos trabajar con texto de manera estructurada.\n",
    "         - **Análisis de texto**: Facilita tareas como conteo de palabras, análisis de sentimientos, y clasificación de texto.\n",
    "         - **Traducción automática**: Ayuda a dividir las palabras para que las máquinas traduzcan partes relevantes.\n",
    "\n",
    "      - **Consideraciones importantes**:\n",
    "         - **Lenguaje y contexto**: Algunos lenguajes, como el chino o japonés, no tienen delimitadores claros como espacios, lo que hace la tokenización más compleja.\n",
    "         - **Signos de puntuación**: Los modelos pueden tratar la puntuación como tokens separados, dependiendo de la configuración.\n",
    "\n",
    "      - **Herramientas comunes**:\n",
    "         - Bibliotecas como **NLTK** o **spaCy** para tokenización tradicional.\n",
    "         - **Tokenizadores específicos de modelos** como Tiktoken de OpenAI o Sentence de Google\n",
    "\n",
    "3. **Lematización/stemmización.** Para algunas aproximaciones, es importante tratar de unificar las palabras que se refieren al mismo concepto. Por ejemplo, si se quiere hacer un conteo simple para crear una nube de palabras de un documento; quisiéramos que corriendo, corrió, y correrá no se tratasen como palabras independientes. Lematización consiste en, dada una forma flexionada (es decir, en plural, en femenino, conjugada, etc.), hallar el [lema](https://es.wikipedia.org/wiki/Lema_(ling%C3%BC%C3%ADstica)) correspondiente. El [lema](https://es.wikipedia.org/wiki/Lema_(ling%C3%BC%C3%ADstica)) es la forma que por convenio se acepta como representante de todas las formas flexionadas de una misma palabra. En nuestro ejemplo: corriendo, corrió, y correrá se transformarían en el verbo en infinitivo *correr*. Una versión simplificada de la lematización es el **stemming** el cual consiste en encontrar la raíz de la palabra removiendo el sufijo o el final de las palabras. Por ejemplo, corrieron y correr se volverían *corr*. \n",
    "   - **Diferencias con la stemmización**\n",
    "      - La lematización **preserva el significado del contexto** y utiliza reglas lingüísticas (como diccionarios morfológicos).\n",
    "      - La **stemmización** simplemente corta palabras a su raíz sin considerar el contexto, lo que puede llevar a errores semánticos.\n",
    "      - Ejemplo: \"amigos\" → \"amig\" (stemmización), pero → \"amigo\" (lematización correcta).\n",
    "\n",
    "   - **Rol en la tokenización y el análisis de texto**\n",
    "      1. **Normalización del texto**:\n",
    "         - La lematización permite tratar palabras con el mismo significado como equivalentes, lo que mejora el análisis.\n",
    "         - Ejemplo: \"corriendo\", \"corro\", \"corrieron\" → Todas se transforman a \"correr\".\n",
    "\n",
    "      2. **Reducción del vocabulario**:\n",
    "         - Al unificar palabras derivadas en sus lemas, se reduce el tamaño del vocabulario, haciendo que los modelos sean más eficientes.\n",
    "\n",
    "      3. **Mejora en las tareas de NLP**:\n",
    "         - En la **clasificación de texto**, agrupa palabras relacionadas, mejorando la representación de datos.\n",
    "         - En **búsqueda semántica**, ayuda a encontrar documentos relevantes aunque usen diferentes formas de una palabra.\n",
    "\n",
    "      4. **Complemento de la tokenización**:\n",
    "         - Después de la tokenización, la lematización refina los tokens para facilitar el procesamiento posterior.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R7tOtOUoUp4Z"
   },
   "source": [
    "## Patrones básicos y Expresiones Regulares\n",
    "\n",
    "Para ilustrar a qué nos referimos, comencemos con un ejemplo sencillo. El patrón más sencillo que se puede utilizar con expresiones regulares es utilizar secuencia de caracteres que uno quiere encontrar en el texto. Por ejemplo, si quisiéramos buscar la palabra *tienda* en un texto, simplemente podríamos usar como patrón `tienda`. Los patrones de búsqueda pueden estar conformados por un solo caracter como `!` para buscar signos de exclamación o también una secuencia de letras:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dZV5xou4Up4Z"
   },
   "source": [
    "<div> <center> \n",
    "\n",
    "| **RE** |      **Ejemplo del patrón capturado**     |\n",
    "|:------:|:-----------------------------------------:|\n",
    "| tienda | El que tenga <u>tienda</u> que la atienda |\n",
    "|    a   |    El que m<u>a</u>druga Dios le ayuda    |\n",
    "|    !   |            ¡Ojo con eso<u>!</u>           |\n",
    "\n",
    "</center> </div>   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8oyQ1cW_Up4a"
   },
   "source": [
    "Este tipo de búsquedas es sensible al uso de mayúsculas, por ejemplo, buscar la palabra `tienda` arroja un resultado diferente al de buscar `Tienda`. Del mismo modo, también es sensible al uso de caracteres especiales como tildes, apostrofes, etc. En la práctica se suelen eliminar estos caracteres especiales para simplificar el texto analizado. Por ejemplo, transformar un texto como:\n",
    "\n",
    "**<center> A palabras necias oídos sordos </center>**\n",
    "\n",
    "por \n",
    "\n",
    "**<center> a palabras necias oidos sordos </center>** \n",
    "\n",
    "hará más sencillo su tratamiento. No obstante, las expresiones regulares son una herramienta superpoderosa y nos permiten usar funciones que simplifican la tarea. Por ejemplo, podemos usar los corchetes (`[]`) para expresar disyunción lógica (`o`). Por ejemplo, la búsqueda `[Tt]ienda` sirve para encontrar la palabra `tienda` **o** la palabra `Tienda`. Los corchetes indican que se busca una palabra que contenga la cadena `ienda` precedida por una letra `t` en minúscula **o** mayúscula. Por ejemplo:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "odgxa6gAUp4b"
   },
   "source": [
    "<div> <center> \n",
    "\n",
    "|    **RE**    |**Patrón capturado**|          **Ejemplo del patrón capturado**            |\n",
    "|:------------:|:----------------:|:------------------------------------------------------:|\n",
    "|   [Tt]ienda  |  Tienda o tienda |        El que tiene <u>tienda</u> que la atienda       |\n",
    "|     [abc]    |   a, b **o** c   | No me <u>a</u>bra los ojos que no le voy a echar gotas |\n",
    "| [1234567890] | Cualquier dígito |            Eramos entre <u>5</u> y 8 personas   |\n",
    "\n",
    "</div> </center> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rTn3TeHeUp4b"
   },
   "source": [
    "Notemos en la última linea que la expresión regular `[1234567890]` nos permite capturar cualquier dígito, no obstante, escribir bloques de dígitos o letras puede ser inconveniente. Es decir, para capturar cualquier letra no es práctico escribir todo el abecedario: `[abcdefghijklmnopqrstuvwxyz]`. En estos casos uno puede completar la búsqueda dentro de corchetes con un guión (`-`) que especifica rangos. Por ejemplo `[0-9]` nos permite capturar cualquier número entre 0 y 9, `[b-g]` nos permite capturar cualquier letra de la `b` a la `g` o sea *b, c, d, e, f **o** g*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SVDXfdXgUp4c"
   },
   "source": [
    "<div> <center>\n",
    "\n",
    "| **RE** |     **Patrón capturado**     |            **Ejemplo del patrón capturado**            |\n",
    "|:------:|:----------------------------:|:------------------------------------------------------:|\n",
    "|  [A-Z] | Cualquier letra en mayúscula |        <u>E</u>l que tiene tienda que la atienda       |\n",
    "|  [a-z] | Cualquier letra en minúscula | N<u>o</u> me abra los ojos que no le voy a echar gotas |\n",
    "|  [0-9] |       Cualquier dígito       |       Eramos entre <u>5</u> y 8 personas       |\n",
    "\n",
    "</div> </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JqLIAaIsUp4c"
   },
   "source": [
    "Podemos también indicar que caracteres no deben ser capturados, para ello utilizamos un caret (`^`) al inicio del corchete `[^]`. Sólo si el caret (`^`) es el primer símbolo dentro del corchete, el patrón subsiguiente es negado. Por ejemplo, `[^a]` significa que se va a capturar cualquier caracter, incluyendo los especiales, excepto la letra *a*. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bO1k6KHNUp4c"
   },
   "source": [
    "<div> <center>\n",
    "\n",
    "| **RE** |               **Patrón capturado**              |            **Ejemplo del patrón capturado**            |\n",
    "|:------:|:-----------------------------------------------:|:------------------------------------------------------:|\n",
    "| [^A-Z] | Cualquier caracter menos una letra en mayúscula |       E<u>l</u> que tiene tienda que la atienda       |\n",
    "|  [^Ss] |     Cualquier caracter excepto una \"s\" o \"S\"    | <u>N</u>o me abra los ojos que no le voy a echar gotas |\n",
    "|  [^.]  |        Cualquier caracter menos un punto        |       <u>E</u>ramos al rededor de 5 a 8 personas       |\n",
    "|  [e^]  |             Captura una \"e\" o un \"^\"            |                       <u>e</u>^x                       |\n",
    "|  [a^b] |                   Captura a^b                   |                 La expresión <u>a^b</u>                |\n",
    "\n",
    "</div> </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FLHWUmf5Up4d"
   },
   "source": [
    "Note sin embargo, que si se usa el caret (`^`) en cualquier otro lugar de la expresión regular, este no va a significar una negación, sino un caret.\n",
    "\n",
    "Del mismo modo, a menudo buscamos capturar patrones opcionales. Por ejemplo, para capturar una palabra en plural o en singular en donde el último caracter es una *s*. Para esto utilizamos el símbolo de pregunta (`?`) después del caracter opcional. El signo de pregunta (`?`) en el contexto de expresiones regulares significa el caracter anterior o ninguno."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HyD7lJ8lUp4d"
   },
   "source": [
    "\n",
    "<div> <center>\n",
    "\n",
    "|  **RE**  | **Patrón capturado** |            **Ejemplo del patrón capturado**           |\n",
    "|:--------:|:--------------------:|:-----------------------------------------------------:|\n",
    "| tiendas? | \"tienda\" o \"tiendas\" |       El que tenga <u>tienda</u> que la atienda       |\n",
    "|  colou?r |  \"color\" o \"colour\"  | Discover the newest hand-picked <u>color</u> palettes |\n",
    "\n",
    "</div> </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dlZpGXyaUp4d"
   },
   "source": [
    "Pero también existen casos donde un caracter se puede repetir más de una vez. Por ejemplo, en un libro se podría encontrar la onomatopeya del mujido de una vaca de diversas formas:\n",
    "\n",
    "**<center> Muu! </center>**\n",
    "\n",
    "**<center> Muuu! </center>** \n",
    "\n",
    "**<center> Muuuu! </center>** \n",
    "\n",
    "**<center> Muuuuu! </center>** \n",
    "\n",
    "A grandes rasgos, podemos describir esta onomatopeya como una palabra que comienza con una *M* seguida con por lo menos dos *u* y finaliza con el signo de exclamación *!*. La expresión regular que nos permite capturar cero o más ocurrencias de un caracter es el asterisco (`*`) también conocido como *cleany star* o *Kleene* . Por ende, la expresión regular `u*` va a capturar tanto `u` como `uuuuuu`, pero a su vez también podría capturar `vaca` pues esta palabra tiene cero letras u. \n",
    "\n",
    "Para corregir esto, podríamos usar la expresión regular `uu*` la cual significa una o más letras u. Algunos patrones más complejos también se pueden utilizar haciendo uso de los corchetes; por ejemplo `[ab]\\*` sirve para capturar cero o más *a*s o *b*s. Por ende, se capturarían textos como *aaaaa*, *bbb* o *ababababab*.\n",
    "\n",
    "Asimismo, para especificar múltiples dígitos podemos usar `[0-9][0-9]*` para capturar cualquier entero. \n",
    "\n",
    "Sin embargo, aún podemos utilizar el signo de suma (`+`), también llamado *Kleene +*, para simplificar las expresiones regulares. El *Kleene +*, nos permite denotar que el caracter a capturar se repite una o más veces. Por ende, la expresión `[0-9]+` es la forma más común de expresar una secuencia de dígitos. Por ejemplo:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TmBl6eTQUp4e"
   },
   "source": [
    "<div> <center>\n",
    "\n",
    "|  **RE**  | **Patrón capturado** |            **Ejemplo del patrón capturado**           |\n",
    "|:--------:|:--------------------:|:-----------------------------------------------------:|\n",
    "   |\n",
    "|   baa*   |  ba con una o más as |               La cabra hace <u>baaa</u>!              |\n",
    "    |   mu+!   | mu! con una o más us |              La vaca hizo <u>muuuuu!</u>           \n",
    "|  [0-9]+  |   Cualquier entero   |              Ese camisa cuesta $<u>25</u>             |\n",
    "\n",
    "</div> </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NslnmvOhUp4e"
   },
   "source": [
    "Otra función importante esta dada por el punto (`.`). Este funciona como comodín o *wildcard*. Esta expresión regular sirve para capturar cualquier caracter excepto los saltos de línea. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yvUqYXX-Up4f"
   },
   "source": [
    "<div> <center>\n",
    "\n",
    "|  **RE**  | **Patrón capturado** |            **Ejemplo del patrón capturado**           |\n",
    "|:--------:|:--------------------:|:-----------------------------------------------------:|\n",
    "   |\n",
    "|   1. | 10 y 1A             | Ganaron el partido <u>18</u> a 2             |\n",
    "|  1.4 | 114 y 1B4            | Vive en el apartamento <u>1B4<u> |\n",
    "\n",
    "</div> </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RPX7OIr-Up4f"
   },
   "source": [
    "También existen los denominado anclas o *anchors* que sirven para capturar elementos en posiciones particulares del texto. Los más comunes son el caret (`^`) y el símbolo de dolar (`$`) los cuales hacen alusión al inicio y final de un texto respectivamente. Por ejemplo, la expresión `^El` solo captura la palabra *El* sólo si está al inicio del corpus de texto. Otras anclas comunes son (`\\b`) y (`\\B`) que denotan los *boundaries* o límites de una palabra o dentro de una palabra respectivamente. Por ejemplo, `\\bel\\b` va a capturar la palabra *el* pero no *elefante*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wVxbECLNUp4f"
   },
   "source": [
    "### Disyunción, agrupación y precedencia\n",
    "\n",
    "\n",
    "A menudo estaremos interesados en buscar más de una palabra a la vez. Por ejemplo, si en nuestro texto quisiéramos buscar países de Latinoamérica,  escribir `[ColombiaPerúChileMéxico...]` sólo nos devolvería alguna de las letras presentes en los corchetes. El operador de disyunción (`|`) sirve para este tipo de casos donde estamos interesados en una u otra palabra, por eso, el patrón `Colombia|Perú|Chile` devuelve Colombia, Perú o Chile. \n",
    "\n",
    "A su vez, puede que nos interesemos no sólo en los países como tal, sino también los gentilicios. Por ejemplo, si queremos extraer Chile o Chilenos necesitamos sofisticar nuestro operador de disyunción para evitar escribir la expresión `Chile|Chilenos`. En este caso podemos utilizar  paréntesis para inidcar que la disyunción sólo aplica para una parte del texto: `Chil(e|enos)`. Note que si omitiéramos los paréntesis `Chile|enos` solo se capturaría *Chile* o *enos* y dado que *Chile* tiene precedencia sobre *enos* en caso de encontrar la palabra *Chilenos* sólo se extraería la primer parte: *Chile* sin el *nos*.\n",
    "\n",
    "Los paréntesis también son un gran complemento para los asteriscos (`*`). Supongamos poseemos el índice de un libro que tiene el siguiente texto: Capítulo 1, Capítulo 2, Capítulo 3, etc. Para capturar todos los capítulos necesitamos crear un patrón que capture repetidamente la palabra *Capítulo* seguida de algún entero. La expresión regular `Capítulo [0-9]+ *` sólo captura *Capítulo* seguida de un entero y 0 más espacios, en este caso necesitamos utilizar los paréntesis: `(Capítulo [0-9]+ *)*`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PS6N_kBqUp4g"
   },
   "source": [
    "### Algunos operadores adicionales\n",
    "\n",
    "<div> <center>\n",
    "\n",
    "| **RE** | **Expansión** |                          **Patrón capturado**                         |\n",
    "|:------:|:-------------:|:---------------------------------------------------------------------:|\n",
    "|   \\d   |     [0-9]     |                            Cualquier dígito                           |\n",
    "|   \\D   |     [^0-9]    |                          Cualquier no dígito                          |\n",
    "|   \\w   |  [a-zA-Z0-9_] |                  Cualquier alfanumérico o guion bajo                  |\n",
    "|   \\W   |     [ˆ\\w]     |                 Cualquier no alfanumérico o guion bajo                |\n",
    "|   \\s   |  [ \\r\\t\\n\\f]  |                           Espacio en blanco                           |\n",
    "|   \\S   |     [ˆ\\s]     |                        No un espacio en blanco                        |\n",
    "|    *   |               |         Cero o más ocurrencias del caracter o expresión pasada        |\n",
    "|    +   |               |         Una o más ocurrencias del caracter o expresión pasada         |\n",
    "|    ?   |               | Exactamente cero o una ocurrencia del del caracter o expresión pasada |\n",
    "|   {n}  |               |            *n* ocurrencias del caracter o expresión pasada            |\n",
    "|  {n,m} |               |        De *n* a *m* ocurrencias del caracter o expresión pasada       |\n",
    "|  {n,}  |               |      Por lo menos *n* ocurrencias del caracter o expresión pasada     |\n",
    "|  {,m}  |               |         Hasta *m* ocurrencias del caracter o expresión pasada         |\n",
    "\n",
    "</div> </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tAIS3-35Up4g"
   },
   "source": [
    "Estos son algunos de los patrones y operadores básicos que vamos a necesitar y utilizar en este curso; sin embargo hay mucho más y los invito a explorarlos por su cuenta. Antes de ilustrar el uso en `Python` es importante recomendar la página https://regexr.com  que permite probar el correcto funcionamiento de las expresiones regulares creadas antes de utilizarlas en el código."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2c3ukuz2Up4g"
   },
   "source": [
    "## Ejemplo en `Python`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-AqdJiKiUp4g"
   },
   "source": [
    "Para ilustrar cómo podemos utilizar expresiones regulares en `Python` vamos a hacer un análisis de texto sencillo sobre el Acuerdo de Paz que firmó el Gobierno de Colombia con el grupo guerrillero FARC-EP en el año 2016. \n",
    "\n",
    "El análisis sencillo que buscamos hacer es armar una nube de palabras para ver cuáles son los términos más frecuentes en el documento.\n",
    "\n",
    "Para ello, incluimos el acuerdo en formato `pdf` en la carpeta `data`. Para cargar archivos `pdf` utilizamos la librería `pdfplumber` y con la función `open` vamos a extraer las páginas del acuerdo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cA75JPKgBBL_"
   },
   "outputs": [],
   "source": [
    "#pip install contexto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uruGeHUcVPIz"
   },
   "outputs": [],
   "source": [
    "#pip install pdfplumber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6i4uET5EUp4g",
    "outputId": "396b5b0b-7497-4389-d181-7633c2dc9b03"
   },
   "outputs": [],
   "source": [
    "# Cargamos pdfplumber\n",
    "import pdfplumber\n",
    "\n",
    "# Procedemos a extraer todo el texto del documento\n",
    "with pdfplumber.open(\"https://raw.githubusercontent.com/ignaciomsarmiento/MALM/blob/main/07_Rec_Descripciones/data/acuerdo_final.pdf\") as pdf:\n",
    "    paginas = pdf.pages\n",
    "    documento = \"\"\n",
    "    for pag in paginas:\n",
    "        text = pag.extract_text()\n",
    "        documento = documento + \" \" + text\n",
    "        \n",
    "# Visualizamos 500 caracteres del texto\n",
    "print(documento[0:500])        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z7oOBS0OUp4j"
   },
   "source": [
    "### Limpieza de texto usando expresiones regulares"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lii88zPBUp4j"
   },
   "source": [
    "Para trabajar con expresiones regulares vamos a utilizar la librería  `re` que cuenta entre sus principales funciones a:\n",
    "\n",
    "* findall: retorna una lista con todos los matchs dentro del texto.\n",
    "* search: retorna un objeto de tipo *match* si existe algún match en el texto.\n",
    "* split: devuelve una lista donde la cadena se ha dividido en cada match.\n",
    "* sub: reemplaza una o más coincidencias con el texto deseado.\n",
    "\n",
    "A su vez, otros paquetes como `Pandas` tienen métodos que permiten hacer uso de expresiones regulares para modificar el objeto. \n",
    "\n",
    "Dado que tenemos el texto en español y nuestro idioma utiliza tildes, esto genera a veces inconvenientes. La librería `unidecode` nos ayuda a solucionar este problema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ec-28QcZkzMv"
   },
   "outputs": [],
   "source": [
    "#pip install unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UWbf3-KBUp4j",
    "outputId": "c9126511-545e-4a1b-feae-0a845a169f27"
   },
   "outputs": [],
   "source": [
    "# Cargamos las librerías a utilizar\n",
    "import re\n",
    "import unidecode\n",
    "\n",
    "# Quitamos tildes\n",
    "documento = unidecode.unidecode(documento)\n",
    "print(documento[0:500]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bJfi1wI-Up4k"
   },
   "source": [
    "Podemos ver que *Ejército* aparecía con tilde ya no la tiene. \n",
    "\n",
    "A continuación usaremos la expresión regular `[^A-Za-z0-9 ]+` para matchear todo lo que no sea un caracter alfanumérico o un espacio y lo reemplazamos con un espacio. Esto quitará comas, guiones y otros caracteres especiales o signos de puntuación. Note la presencia del caret `^` al inicio de los corchetes cuadrados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cUU0Jf39Up4k",
    "outputId": "c2f55209-ddea-4454-d49b-c661baef7433"
   },
   "outputs": [],
   "source": [
    "documento = re.sub('[^A-Za-z0-9 ]+', ' ', documento)\n",
    "print(documento[0:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AGiotTnsUp4k"
   },
   "source": [
    "A continuación ponemos todo en minúscula y con un sólo espacio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LWHg8Gi_Up4k",
    "outputId": "5cdd31e6-3053-47b8-9868-bf806ec72ca9"
   },
   "outputs": [],
   "source": [
    "# Minúsculas\n",
    "documento = documento.lower()\n",
    "# Espacios\n",
    "documento = re.sub('\\s+', ' ', documento)\n",
    "print(documento[0:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sxfi55FkUp4l"
   },
   "source": [
    "Puesto que nos interesa ver la frecuencia de palabras, eliminaremos todos los números y espacios no simples que se hayan generado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xlFCiaCtUp4l",
    "outputId": "7604f221-8754-4c81-d484-939d77c1571c"
   },
   "outputs": [],
   "source": [
    "documento = re.sub(\"\\d+\", \"\", documento)\n",
    "documento = re.sub('\\s+', ' ', documento)\n",
    "documento = documento.strip()\n",
    "print(documento[0:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DglfExCyUp4l"
   },
   "source": [
    "Note que la expresión regular `'\\s+'` hace alusión a que se matcheen todo tipo de espacios en blanco 1 o más veces. Estos incluyen tabs, saltos a nuevas líneas, entre otros. Esta expresión regular es mucho más completa que `' +'`, que sólo identifica los espacios en blanco que separan palabras dentro de una misma línea."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2d9nyW3oUp4l"
   },
   "source": [
    "### Tokenización"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "exSq-uNQUp4m"
   },
   "source": [
    "El texto ahora está lo suficientemente limpio, pero para determinar la frecuencia de palabras debemos\n",
    "separar los términos. Para ello vamos a tokenizarlo, es decir a partirlo en palabra por palabra; luego vamos a eliminar palabras redundantes o stopwords, y finalmente vamos lemmatizar el texto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FnALxcBaUp4m",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!python -m spacy download es_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f-h12k5MUp4m",
    "outputId": "62fd4f2a-0c8d-4373-e170-1f598e97dcec",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Cargamos spacy\n",
    "import spacy\n",
    "\n",
    "# Utilizamos spacy para tokenizar el documento\n",
    "nlp = spacy.load(\"es_core_news_sm\")\n",
    "\n",
    "doc = nlp(documento)\n",
    "\n",
    "print(doc[0:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I1A2iiyOUp4n",
    "outputId": "e3dde6ba-f5ba-4322-bd04-9cd4cf72559a"
   },
   "outputs": [],
   "source": [
    "type(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "no81dn9EEoJo"
   },
   "source": [
    "# Uso de la libreria del DNP para NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8FtgnygqEq6h"
   },
   "source": [
    "La librería de procesamiento y análisis de texto, [ConTexto](https://github.com/ucd-dnp/ConTexto), tiene como objetivo principal proporcionar herramientas que simplifiquen las tareas y proyectos que involucren procesamiento y análisis de texto. La librería fue desarrollada en el lenguaje de programación de Python y contiene un conjunto de funciones que permiten realizar transformaciones y análisis de textos de forma simple, utilizando diferentes técnicas para lectura y escritura de archivos de texto, incluyendo reconocimiento óptico de caracteres (OCR), limpieza de textos y remoción de palabras no deseadas para el análisis (stop words), traducción y corrección de textos, generación de nubes de palabras, cálculo de similitudes entre textos, entre otras."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I8RoVqc2Up4n"
   },
   "source": [
    "El próximo paso es eliminar lo que se conoce como las *stopwords*, que son aquellas palabras que no le añaden ningún significado al texto, por ejemplo: el, la, y, o, del, con, a, etc. Para ello, ahora sí, utilizamemos `nltk` que posee una lista de los stopwords por idioma. \n",
    "\n",
    "Vamos a descargar las del idioma español y a ponerlas en el formato acorde a nuestro documento. (Si este camino arroja error, podés importar *nltk* (`import nltk`) y ejecutar (`nltk.download()`) se abrirá un menú donde podés instalar el corpus de *stopwords*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dqRk93_8Up4n",
    "outputId": "0e52ba29-1ff7-4c4f-899c-fb28ecc5ca4c"
   },
   "outputs": [],
   "source": [
    "with open('https://raw.githubusercontent.com/ignaciomsarmiento/MALM/blob/main/07_Rec_Descripciones/data/sw_DNP.txt', 'r', encoding='latin-1') as archivo:\n",
    "    lista_strings = archivo.readlines()\n",
    "\n",
    "lista_strings = [string.strip() for string in lista_strings]\n",
    "print(lista_strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Nm6BU-YwUp4n"
   },
   "outputs": [],
   "source": [
    "# Eliminamos stopwords\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stopwords = set(stopwords.words(\"spanish\"))\n",
    "\n",
    "# Creamos un diccionario de stopwords en español\n",
    "stopwords = [unidecode.unidecode(i) for i in stopwords]\n",
    "stopwords = set(stopwords)\n",
    "\n",
    "doc2 = [tok for tok in doc if str(tok) not in stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 174
    },
    "collapsed": true,
    "id": "ROIC2MsmUp4n",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "a4a7eb07-9402-48ee-8e9e-93d8d6a603e8",
    "tags": []
   },
   "outputs": [],
   "source": [
    "doc = limpieza_texto(documento, quitar_numeros=False, n_min=3, lista_palabras=lista_strings)\n",
    "doc = remover_stopwords(doc, lista_expresiones=['project gutenberg'])\n",
    "doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XwbDUfaIUp4o"
   },
   "source": [
    "Con el documento tokenizado y eliminadas las *stopwords*, el siguiente paso es lematizarlo para unificar las palabras que se refieren al mismo concepto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VRoSwzn6Up4o",
    "outputId": "fd211ffa-cd3c-441b-be62-f886fb21af13",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Lematización con librería Spacy\n",
    "doc2 = lematizar_texto(doc)\n",
    "\n",
    "print(doc2[0:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4oFWnSCfUp4o",
    "outputId": "a2e2334a-d51a-4721-fd31-59ad3e2947be"
   },
   "outputs": [],
   "source": [
    "texto_lematizado = lematizar_texto(doc, libreria='stanza')\n",
    "print(texto_lematizado[0:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 174
    },
    "collapsed": true,
    "id": "jEi2Un7sUp4o",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "79a85f2f-24a7-41a4-f2a2-b03f4176967b",
    "tags": []
   },
   "outputs": [],
   "source": [
    "doc2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mzUrl5oSUp4p"
   },
   "source": [
    "Luego, encontraremos las raíces de las palabras (*stemming*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WmU4GCVBUp4p",
    "outputId": "47bc9727-cc87-450d-f145-9bb2d89896a3"
   },
   "outputs": [],
   "source": [
    "stem = stem_texto(doc2, 'auto')\n",
    "print(stem[0:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G2nFFESYUp4p",
    "outputId": "84575b5c-5edd-477a-8072-e2c568b31807"
   },
   "outputs": [],
   "source": [
    "\n",
    "import nltk\n",
    "from nltk import SnowballStemmer\n",
    "spanishstemmer = SnowballStemmer(\"spanish\")\n",
    "\n",
    "stems = [spanishstemmer.stem(token) for token in lemmas]\n",
    "\n",
    "\" \".join(stems[0:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HttJBOJfUp4p"
   },
   "source": [
    "Una desventaja del stemming es que sus algoritmos son más simples que los de lematización. Pueden recortar demasiado la raíz y encontrar relaciones entre palabras que realmente no existen (*overstemming*). También puede suceder que deje raíces demasiado extensas o específicas, y que tengamos más bien un déficit de raíces (*understemming*); en cuyo caso, las palabras que deberían convertirse en una misma raíz no lo hacen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sVrLAc4wUp4q"
   },
   "source": [
    "Con nuestro texto \"limpio\", podemos ver la frecuencia de palabras y así visualizar  los conceptos más importantes del acuerdo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 634
    },
    "id": "BJHGQNwxUp4q",
    "outputId": "b26472b5-9712-4057-e300-5b0a4724d87d"
   },
   "outputs": [],
   "source": [
    "par_nubes(stem, n1=1, n2=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 719
    },
    "id": "Nr9btq06Up4q",
    "outputId": "72a435c6-05c7-4c48-bee1-501feee1e024"
   },
   "outputs": [],
   "source": [
    "grafica_barchart_frecuencias(stem, dim_figura=(7,4), titulo='Frecuencias de palabras', ubicacion_archivo='barras_palabras.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 637
    },
    "id": "EdGJtXFPH_VX",
    "outputId": "ca665f50-aeb3-40fa-a5f3-62bdabf69d31"
   },
   "outputs": [],
   "source": [
    "# Solo se cuenta la coocurrencia si las palabras están a 5 o menos palabras entre sí\n",
    "mat_ven = matriz_coocurrencias(stem, max_num=60, modo='ventana', ventana=5)\n",
    "\n",
    "## Graficar co-ocurrencias de palabras en el texto\n",
    "graficar_coocurrencias(mat_ven, dim_figura=(12,7), graficar=True, seed = 11,\n",
    "                      offset_y =0.13, vmin= 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ANJ7M_-xFr2L"
   },
   "source": [
    "# Corrección de texto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3JCTrcxVG0OG"
   },
   "source": [
    "La libreria del DNP nos permite usar una función para corregir texto y esto puede ser muy utíl cuando trabajamos en encuestas o bases de datos que puedan tener errores ortograficos para que los modelos no lean esa palabra mal escrita como otra palabra a la que no se referian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IfNOP8OsFtX0",
    "outputId": "d956b523-72c7-4db8-c80d-32d02865e084"
   },
   "outputs": [],
   "source": [
    "texto = 'Ojalá halya un buen asado de polllo, para poder comer delizioso.'\n",
    "texto_corregido = corregir_texto(texto)\n",
    "print(texto)\n",
    "print(\"------------\")\n",
    "print(texto_corregido)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YVLotpMFUp4y"
   },
   "source": [
    "# Referencias\n",
    "\n",
    "- Bird, S., Klein, E., & Loper, E. (2009). Natural language processing with Python: analyzing text with the natural language toolkit. \" O'Reilly Media, Inc.\".\n",
    "\n",
    "- Jurafsky, D., &; Martin, J. H. (2020). Speech and language processing: An introduction to natural language processing, computational linguistics, and speech recognition. Pearson.\n",
    "\n",
    "- Weizenbaum, J. 1966. ELIZA – A computer program for the study of natural language communication between man and machine. CACM, 9(1):36–45.\n",
    " \n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
