{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dJptJunyHyUM"
      },
      "source": [
        "<div >\n",
        "<img src = \"../Machine Learning Aplicado al Marketing-4875-x-834.jpg\" />\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B5Ec05pLHyUS"
      },
      "source": [
        "# Sistemas de Recomendación basado en Contenidos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B3yYBULn4gyn"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ignaciomsarmiento/RecomSystemsLectures/blob/main/L06_basados_contenidos/L06_Content_clase.ipynb)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bYnUE46bHyUT"
      },
      "source": [
        "Este *cuaderno* trata sobre filtrado colaborativo basado en contenidos. El objetivo del *cuaderno* es que usted obtenga una visión general del problema predictivo de los sistemas de recomendación que utilizan filtrado colaborativo basado en contenidos, aprenda distintos algoritmos que lo implementan, y que sea capaz de reconocer sus características, funcionamiento, y  desarrollarlos en `Python`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Swmw332zHyUT"
      },
      "source": [
        "## Introducción\n",
        "\n",
        "Los sistemas basados en contenido, a diferencia de los filtros colaborativos, no requieren datos relacionados a otros individuos u actividades pasadas. Por el contrario, estos brindan recomendaciones basadas en el perfil del usuario y los metadatos que se tiene sobre elementos particulares.\n",
        "\n",
        "Si bien en este tipo de sistemas se utilizan la interacción entre un usuario y un ítem particular (lo compró, calificó, etc.) nosotros nos centraremos en el uso de texto asociado a los ítems. En este cuaderno en particular se abordará la construcción de dos tipos de recomendadores basados en contenido de películas, pero que pueden ser aplicados a otros productos:\n",
        "\n",
        " 1. Recomendador basado en la descripción de la trama: este modelo compara las descripciones de diferentes películas y proporciona recomendaciones basado en películas con tramas similares.\n",
        " 2. Recomendador basado en metadatos: este modelo tiene en cuenta una gran cantidad de características, como géneros, palabras clave, elenco, director, etc. A partir de ellos proporciona recomendaciones que son las más similares basadas en estas características.\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9UkXOzYWHyUU"
      },
      "source": [
        "## Texto como datos\n",
        "\n",
        "Para poder utilizar el texto asociado a los ítems debemos primero transformarlo en datos que puedan ser utilizado por los sistemas de recomendación.\n",
        "\n",
        "### Limpieza de datos\n",
        "\n",
        "Antes de poder utilizar el texto necesitamos transformarlo de forma tal que las maquinas puedan utilizarlo. Hay al menos 3 pasos previos:\n",
        "\n",
        "1. **Limpieza de texto** **Las expresiones regulares (regex)** juegan un papel importante al automatizar tareas específicas de limpieza.\n",
        "\n",
        "2. **Tokenización.**\n",
        "\n",
        "3. **Lematización/stemmización.**\n",
        "\n",
        "\n",
        "### Vectorización\n",
        "\n",
        "La idea es que utilicemos estos datos de forma tal que podamos construir medidas de similitud entre ellos. Dicho de otra forma, si tenemos tres películas A, B y C; ¿Cómo medimos cuán similares son la trama de A con la de B o la de C?\n",
        "\n",
        "El primer paso entonces es representar este texto como  vectores. En otras palabras, cada documento se representa como una serie de n números, donde cada número representa una dimensión y n es el tamaño del vocabulario de todos los documentos juntos. Donde los valores de los vectores van a depender del vectorizador que utilizamos. Los dos vectorizadores más populares son `CountVectorizer` y `TF-IDFVectorizer`.\n",
        "\n",
        "#### CountVectorizer\n",
        "\n",
        "El `CountVectorizer` es la forma más sencilla de vectorizar el texto. Para explicarlo utilizaremos un ejemplo. Supongamos que tenemos los siguientes 3 documentos, A, B, y C:\n",
        "\n",
        "  - A: El sol es una estrella.\n",
        "  - B: Un buen viajante no tiene planes.\n",
        "  - C: Juan tiene una mascota nueva\n",
        "  \n",
        "Transformemos estos documentos en su forma vectorial utilizando `CountVectorizer`. El primer paso es encontrar el vocabulario para este grupo de documentos. Esto consiste en encontrar el número de palabras únicas: el, sol, es, una, estrella, un, buen, viajante, no, tiene, planes, Juan, mascota, nueva. Es decir, tokenizamos el texto y contamos el número de ocurrencias, en este caso el vocabulario es de 14 palabras.\n",
        "\n",
        "Es común, como vimos en el *cuaderno Expresiones Regulares*, eliminar las stopwords. Por lo que, si hacemos esto, el tamaño del vocabulario se reduce a:  buen, estrella, Juan, mascota, nueva, planes, sol, viajante. Es decir, el tamaño ahora es 8.\n",
        "\n",
        "Esto implica que los documentos se van a representar como vectores con 8 dimensiones, donde cada dimensión representa el número  de veces que la palabra ocurre:\n",
        "\n",
        "- A:(0, 1, 0, 0, 0, 0, 1, 0)\n",
        "- B:(1, 0, 0, 0, 0, 1, 0, 1)\n",
        "- C:(0, 0, 1, 1, 1, 0, 0, 0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C_TPj80-HyUV"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "ejemplo = pd.read_csv(\"https://raw.githubusercontent.com/ignaciomsarmiento/datasets/main/example_text.csv\", sep=\";\")\n",
        "ejemplo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KJo-2mBgHyUY"
      },
      "outputs": [],
      "source": [
        "# Import CountVectorizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "#Definimos un objeto CountVectorizer y creamos los vectores\n",
        "count = CountVectorizer()\n",
        "count_matrix = count.fit_transform(ejemplo['text'])\n",
        "count_matrix.toarray()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ycu5QbypJkme"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9F2gPt3DHyUZ"
      },
      "outputs": [],
      "source": [
        "# Descargamos las stopwords\n",
        "from nltk.corpus import stopwords\n",
        "lista_stopwords = stopwords.words(\"spanish\")\n",
        "len(lista_stopwords)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FGEb1REWHyUa"
      },
      "outputs": [],
      "source": [
        "#Definimos un objeto CountVectorizer y creamos los vectores\n",
        "count = CountVectorizer(stop_words= lista_stopwords)\n",
        "count_matrix = count.fit_transform(ejemplo['text'])\n",
        "count_matrix.toarray()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hcobKraOHyUb"
      },
      "source": [
        "#### TF-IDFVectorizer\n",
        "\n",
        "Sin embargo, podemos pensar que no todas las palabras en un documento pesan lo mismo. Por ejemplo, un documento sobre vacas contendrá frecuentemente el termino vaca. Por lo tanto, probablemente la presencia de esta palabra no sea tan informativa como algún otra.\n",
        "\n",
        "El TF-IDFVectorizer (term frequency-inverse document frequency) incorpora esta noción  entonces el $tf-idf_{ij}$ de una palabra $i$ en el documento $j$ esta dado por:\n",
        "\n",
        "$$\n",
        "tf-idf_{ij}=tf_{ij} \\times \\left( \\log \\left( \\frac{1+N}{1+df_i} \\right)+1\\right)\n",
        "$$\n",
        "\n",
        "donde:\n",
        "\n",
        "- $tf_{ij}$ es la frecuencia palabra $i$ en el documento $j$\n",
        "- $df_{ij}$ es el número de documentos que contienen la palabra $i$\n",
        "- $N$ es el número de documentos\n",
        "\n",
        "Los vectores $tf-idf_{ij}$ resultantes luego son normalizados por la norma euclideana. Al hacer esta transformación tendremos que el peso de una palabra será mayor si aparece con más frecuencia o si está presente en menos documentos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "prdLgWKqHyUb"
      },
      "source": [
        "## Recomendador basado en trama\n",
        "\n",
        "Estamos ahora en condiciones de armar una función que recomiende películas y/o series basado en la descripción de la trama. Para ello utilizaremos datos de películas y series recientes que provienen del sitio www.filmaffinity.com:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r_CDJ1zHHyUb"
      },
      "outputs": [],
      "source": [
        "# Cargamos las librerías a utilizar\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Cargamos los datos\n",
        "pelis = pd.read_csv('https://raw.githubusercontent.com/ignaciomsarmiento/MALM/refs/heads/main/08_Rec_contenidos/data/pelis_recommend.csv', sep=',')\n",
        "\n",
        "pelis.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jElcmJIjHyUc"
      },
      "outputs": [],
      "source": [
        "pelis.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VHxi0s50HyUc"
      },
      "source": [
        "Esta base cuenta con 1321 películas/series y 14 características. Por ahora utilizaremos el titulo y la descripción de la trama que se encuentra en la columna sinopsis. Los pasos para construir serán los siguientes:\n",
        "\n",
        "1. Prepararemos los datos creando vectorizando la sinopsis usando TF-IDF.\n",
        "2. Calcularemos cuán similares son estos vectores usando la distancia de coseno.\n",
        "3. Escribiremos una función que toma como argumento el título de la película/serie y devuelve aquellas más similares.\n",
        "\n",
        "### Preparando los datos\n",
        "\n",
        "Antes de vectorizar necesitamos limpiar y homogenizar el texto que aparece en las sinopsis. Vamos a proceder de manera similar a como lo hicimos en el *cuaderno: Expresiones Regulares*, adaptandonos a las particularidades de las descripciones de las películas/series.\n",
        "\n",
        "Iniciemos tomando la descripción de una serie y avancemos paso a paso viendo como el texto se va modificando con cada transformación."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ztwoop3PHyUd"
      },
      "outputs": [],
      "source": [
        "#Escogemos la serie de HBO: Sustos ocultos de Frankelda\n",
        "example=pelis['sinopsis'][100]\n",
        "example"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bYH2X1wfHyUd"
      },
      "source": [
        "Carguemos las librerías a utilizar y creemos una lista de stopwords:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vj3wdk_ZIUAr"
      },
      "outputs": [],
      "source": [
        "pip install unidecode"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4NgqY8h-ImGo"
      },
      "outputs": [],
      "source": [
        "!python -m spacy download es_core_news_sm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q4uQh1gxHyUd"
      },
      "outputs": [],
      "source": [
        "# Cargamos las librerías a utilizar\n",
        "import unidecode\n",
        "import regex\n",
        "import spacy\n",
        "nlp = spacy.load(\"es_core_news_sm\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E-rJRRptHyUe"
      },
      "source": [
        "`nltk` trae una lista de 313 *stopwords* comúnes del español, pero las extenderemos con unas adicionales obtendias del libro de Fradejas Rueda (2020) disponible en la carpeta `data`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NEF1PxpVHyUe"
      },
      "outputs": [],
      "source": [
        "# Cargamos extra stop words\n",
        "extra_stopwords = pd.read_csv('https://raw.githubusercontent.com/ignaciomsarmiento/MALM/refs/heads/main/08_Rec_contenidos/data/stopword_extend.csv', sep=',')\n",
        "extra_stopwords= extra_stopwords['palabra'].to_list()\n",
        "lista_stopwords = np.concatenate((lista_stopwords, extra_stopwords))\n",
        "lista_stopwords=np.unique(lista_stopwords)\n",
        "len(lista_stopwords)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lM0jFeNpHyUf"
      },
      "source": [
        "Esto nos permite ampliar en casi 200 stopwords nuestra lista. Quitemos los caracteres especiales del español, por ejemplo *gruñón* se volverá *grunon*:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nAijWiQ2HyUf"
      },
      "outputs": [],
      "source": [
        "example = unidecode.unidecode(example)\n",
        "print(example)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nJYzQmusHyUf"
      },
      "source": [
        "Tokenizamos el texto utilizando los espacios en blanco. Esto facilitará el proceso de limpieza."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "34uOBDV1HyUf"
      },
      "outputs": [],
      "source": [
        "example = example.split(\" \")\n",
        "print(example)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nphhm_DTHyUg"
      },
      "source": [
        "Buscamos si existe algún link a pagina web y los remplazamos por la palabra URL. En este caso del ejemplo no esta presente, pero es común que aparezca y no queremos que este tipo de texto nos \"ensucie\" el análisis:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BGFQ_w9sHyUg"
      },
      "outputs": [],
      "source": [
        "example = ['URL' if bool(regex.search(\"http[s]?://\", i)) else i for i in example]\n",
        "print(example)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IYhxw8LOHyUg"
      },
      "source": [
        "Remplazamos URL por espacios vacios utilizando expresiones regulares:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hP95RzWkHyUg"
      },
      "outputs": [],
      "source": [
        "example = [i for i in example if i not in [\"URL\",\"\"]]\n",
        "print(example)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a8XSMNVjHyUh"
      },
      "source": [
        "Remplazamos símbolos de puntuación y aquellos que denotan una nueva linea (`\\n`) por espacios vacios:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nEhpiiUhHyUh"
      },
      "outputs": [],
      "source": [
        "example = [regex.sub(\"[^\\\\w\\\\s]|\\n\", \"\", i) for i in example]\n",
        "print(example)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ieS8eWbHyUh"
      },
      "source": [
        "Remplazamos números por espacios vacios:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XAQcitybHyUh"
      },
      "outputs": [],
      "source": [
        "example = [regex.sub(\"^[0-9]*$\", \"\", i) for i in example]\n",
        "print(example)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JTSKanPgHyUi"
      },
      "source": [
        "Ponemos todo en minúscula:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jMvvzGbxHyUi"
      },
      "outputs": [],
      "source": [
        "example = [ i.lower() for i in example]\n",
        "print(example)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FqK7gK0gHyUi"
      },
      "source": [
        "Dos pasos adicionales particulares de este texto. Quitaremos la palabra `tv` y remplazaremos `miniserie` por `miniseriedetv`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "42EekbGtHyUi"
      },
      "outputs": [],
      "source": [
        "example = [regex.sub(\"tv\", \"\", i) for i in example]\n",
        "example = [regex.sub(\"miniserie\", \"miniseriedetv\", i) for i in example]\n",
        "print(example)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uj5lIsT9HyUj"
      },
      "source": [
        "Eliminamos las *stopwords*:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Md0PhKBHHyUj"
      },
      "outputs": [],
      "source": [
        "example = [i for i in example if i not in lista_stopwords]\n",
        "print(example)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8hhTvF3zHyUj"
      },
      "source": [
        "Unimos la lista de palabras para simplificar la lematización:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W-KbCo95HyUj"
      },
      "outputs": [],
      "source": [
        "example = ' '.join(example)\n",
        "print(example)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fc0ateQ0HyUj"
      },
      "source": [
        "Lematizamos:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kqp7Ohg5HyUk"
      },
      "outputs": [],
      "source": [
        "example = nlp(example)\n",
        "example = [x.lemma_ for x in example]\n",
        "print(example)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w7obMhWQHyUk"
      },
      "source": [
        "Notemos que el lematizador transforma ciertas palabras de forma interesante. Por ejemplo, *enfrentarse* se vuelve *enfrentar él*. Removeremos entonces este `él`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ag4KtBiBHyUk"
      },
      "outputs": [],
      "source": [
        "example = [regex.sub(\"él\", \"\", i) for i in example]\n",
        "print(example)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sg8a3-q2HyUk"
      },
      "source": [
        "La última transformación será quitar aquellas palabras muy cortas, que tengan menos de 5 caracteres."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BH1n4WlmHyUk"
      },
      "outputs": [],
      "source": [
        "example = [i for i in example if len(i) >= 5]\n",
        "print(example)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0lKvtGgqHyUl"
      },
      "source": [
        "Quedo entonces \"limpio\" nuestro ejemplo. Te invito a probar como cambian los resultados quitando o modificando estos pasos.\n",
        "\n",
        "Ahora que tenemos claro como se puede proceder tenemos que aplicar esto a todas las descripciones de las tramas de las películas/series en nuestra base. Creemos entonces una función que condense estos pasos:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h1pTh9B8HyUl"
      },
      "outputs": [],
      "source": [
        "def sinopsis_cleaning(txt):\n",
        "\n",
        "    out = unidecode.unidecode(txt)\n",
        "    out = out.split(\" \")\n",
        "    out = ['URL' if bool(regex.search(\"http[s]?://\", i)) else i for i in out]\n",
        "    out = [i for i in out if i not in [\"URL\",\"\"]]\n",
        "    out = [regex.sub(\"[^\\\\w\\\\s]|\\n\", \"\", i) for i in out]\n",
        "    out = [regex.sub(\"^[0-9]*$\", \"\", i) for i in out]\n",
        "    out = [ i.lower() for i in out]\n",
        "    out = [regex.sub(\"tv\", \"\", i) for i in out]\n",
        "    out = [regex.sub(\"miniserie\", \"miniseriedetv\", i) for i in out]\n",
        "    out = [i for i in out if i not in lista_stopwords]\n",
        "    out = ' '.join(out)\n",
        "    out = nlp(out)\n",
        "    out = [x.lemma_ for x in out]\n",
        "    out = [regex.sub(\"él\", \"\", i) for i in out]\n",
        "    out = [i for i in out if len(i) >= 5]\n",
        "\n",
        "    return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lWtNExoHHyUl"
      },
      "source": [
        "Aplicamos la función"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CkoHZJcGHyUl"
      },
      "outputs": [],
      "source": [
        "clean = list(map(sinopsis_cleaning, pelis['sinopsis']))\n",
        "clean_sentences = [\" \".join(i) for i in clean]\n",
        "print(clean_sentences[100])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zIqrQTa1HyUm"
      },
      "source": [
        "Con las descripciones de la trama limpias crearemos el vector TF-IDF. Para ello importaremos el objeto [TfidfVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) de [scikit](https://scikit-learn.org/stable/) que nos va a permitir crear la matriz con los documentos vectorizados. Pero como paso intermedio quitaremos las *stopwords*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "alxz2MtdHyUm"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "#Definimos el objeto TF-IDF Vectorizer Object\n",
        "tfidf = TfidfVectorizer()\n",
        "\n",
        "#Construimos la matriz TF-IDF\n",
        "tfidf_matrix = tfidf.fit_transform(clean_sentences)\n",
        "\n",
        "#Dimensiones de la matriz\n",
        "tfidf_matrix.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rgz7I2TsHyUm"
      },
      "source": [
        "Es decir, hemos creado  vectores para la sinopsis de cada película/serie. El paso siguiente es crear la distancia de coseno con cada película. para ello vamos a crear una matriz donde la celda $i$ y la columna $j$ represente la similaridad que hay entre $i$ y $j$. Esta será una matriz simétrica con elementos en la diagonal principal igual a 1, puesto que la similitud entre una película/serie y si misma es 0.\n",
        "\n",
        "\n",
        "### Similaridad entre las tramas\n",
        "\n",
        "Dado que vectorizamos TF-IDF, la norma de estos siempre será 1. Esto simplifica el cómputo de la disimilaridad de coseno ya que se resume a calcular el producto punto.  La librería [scikit](https://scikit-learn.org/stable/)  cuenta con una función que nos permite computar de manera sencilla esta matriz con la función `linear_kernel`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yP0gmW8SHyUm"
      },
      "outputs": [],
      "source": [
        "# Cargamos las funciones a utilizar\n",
        "from sklearn.metrics.pairwise import linear_kernel\n",
        "\n",
        "# Calculamos el producto punto\n",
        "cosine_sim = linear_kernel(tfidf_matrix, tfidf_matrix)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TB_Vi2lfHyUn"
      },
      "source": [
        "### Función recomendadora\n",
        "\n",
        "Estamos ahora en condiciones de crear una función que toma como insumo el título de la película/serie, y devuelve recomendaciones. Realizaremos los siguientes pasos en la construcción de la función de recomendación:\n",
        "\n",
        "1. Declaramos el título de la película como argumento.\n",
        "2. Crearemos una serie que contenga como índice el título de la película/serie y el valor es el correspondiente índice de la serie original:\n",
        "3. Obtenemos el índice de la película a partir de la asignación inversa de índices del paso anterior\n",
        "4. Obtenemos la similitud de coseno para esa película en particular con todas las películas. Convertimos esto en una lista de tuplas donde el primer elemento es la posición y el segundo es la similitud.\n",
        "5. Obtenemos esta lista de tuplas sobre la base de la similitud.\n",
        "6. Obtenemos los 10 elementos top de esta lista ignorando el primer elemento ya que será la puntuación consigo misma.\n",
        "7. Retornamos los títulos recomendados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eJUYsBVbHyUn"
      },
      "outputs": [],
      "source": [
        "def recomendador(title, cosine_sim=cosine_sim, df=pelis):\n",
        "\n",
        "    #Paso 2\n",
        "    df = df.reset_index()\n",
        "    indices = pd.Series(df.index, index=df['titulo']).drop_duplicates()\n",
        "    #Paso 3\n",
        "    idx = indices[title]\n",
        "\n",
        "    #Paso 4\n",
        "    sim_scores = list(enumerate(cosine_sim[idx]))\n",
        "\n",
        "    #Paso 5\n",
        "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    #Paso 6\n",
        "    sim_scores = sim_scores[1:11]\n",
        "\n",
        "    movie_indices = [i[0] for i in sim_scores]\n",
        "\n",
        "    #Paso 7\n",
        "    return pelis['titulo'].iloc[movie_indices]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xzamoyyWHyUn"
      },
      "source": [
        "Generemos entonces recomendaciones para la serie colombiana \"Las iguanas\", pero antes veamos la sinopsis:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9YnLkXg4HyUn"
      },
      "outputs": [],
      "source": [
        "pelis[pelis['titulo']=='Las iguanas']['sinopsis'].values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "djuAoSikHyUn"
      },
      "source": [
        "Este es un drama interesante que toca temas sensibles. Veamos las recomendaciones que nos genera"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B2P66GguHyUn"
      },
      "outputs": [],
      "source": [
        "#Obtenemos las recomendaciones\n",
        "recomendador('Las iguanas')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CiSpbl4rHyUo"
      },
      "source": [
        "Podemos ver que la función está generando recomendaciones de películas y miniseries que envuelven grupo de individuos y drama. Visualicemos utilizando una nube de palabras cuáles son los términos más frecuentes en las descripciones:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y5eU_diLHyUo"
      },
      "outputs": [],
      "source": [
        "# Cargamos las librerías a utilizar\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#ponemos en un vector las peliculas recomendadas\n",
        "pelis_rec=pelis[pelis['titulo'].isin(recomendador('Las iguanas'))]\n",
        "\n",
        "#Limpiamos el texto\n",
        "text = \" \".join(pelis_rec['sinopsis'])\n",
        "text = sinopsis_cleaning(text)\n",
        "text=' '.join(text)\n",
        "\n",
        "# Armamos la nube de palabras\n",
        "wordcloud = WordCloud(width = 1600, height = 800, stopwords = lista_stopwords,\n",
        "    background_color = \"white\").generate(text)\n",
        "plt.figure(figsize = (20, 10))\n",
        "plt.imshow(wordcloud, interpolation = 'bilinear')\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nc_qsVitHyUo"
      },
      "source": [
        "La nube de palabras confirma la intuición de que el recomendador está sugiriendo series sobre grupos de amigos y la trama contiene secretos, poder y abuso."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RAU902VkHyUo"
      },
      "source": [
        "## Recomendador basado en metadatos\n",
        "\n",
        "Para construir el recomendador basado en metadatos seguiremos una estrategia similar a la de la sección previa, sin embargo, cambiaremos los datos que sirven como insumo para generar las recomendaciones.\n",
        "\n",
        "Seguiremos utilizando la base de datos de películas y series que introdujimos en la sección anterior. El foco lo pondremos sobre los metadatos de estas películas/series, específicamente sobre el género, el reparto y la dirección."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6BFl5VwHHyUo"
      },
      "outputs": [],
      "source": [
        "pelis = pd.read_csv('https://raw.githubusercontent.com/ignaciomsarmiento/RecomSystemsLectures/main/L06_basados_contenidos/data/pelis_recommend.csv', sep=',')\n",
        "pelis[['titulo','genero','reparto','direccion']].head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3o8K-ZO0HyUp"
      },
      "source": [
        "Podemos ver que potencialmente hay información valiosa, pero es necesario \"limpiar\" estos datos. En el caso de nombres necesitamos limpiarlos de forma tal que el vectorizador trate a dos individuos con el mismo primer nombre como individuos distintos; por ejemplo, Diego Peretti y Diego Cremonesi. El vectorizador tratará a ambos Diegos como sí fuesen el mismo individuo lo que puede afectar las recomendaciones.\n",
        "\n",
        "### Preparando los datos\n",
        "\n",
        "En esta sección limpiaremos los datos de manera levemente diferente a como lo hicimos en la sección anterior para mostrar que hay múltiples caminos y formas de hacer esta tarea, como así también la versatilidad de `Python`.\n",
        "\n",
        "Comencemos limpiando el género. Para ello primero remplazaremos las barras verticales pondremos todo en minúsculas y unificaremos términos, por ejemplo \"Serie de TV\" se volverá *seriedetv*."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u2rTVtQuHyUp"
      },
      "outputs": [],
      "source": [
        "#Remplazamos barras verticales por puntos\n",
        "pelis['genero']=pelis['genero'].str.replace(\"|\",\"\",regex=True)\n",
        "pelis[['titulo','genero','reparto','direccion']].head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Creamos una lista de términos delimitados por puntos\n",
        "pelis['genero']=pelis['genero'].str.split(\".\")\n",
        "pelis[['titulo','genero','reparto','direccion']].head()"
      ],
      "metadata": {
        "id": "YFIHTjAe5mMa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BTsGKe7eHyUp"
      },
      "source": [
        "Con los géneros en una lista crearemos una función limpiadora que nos permitirá normalizar el texto en español, remover espacios en blanco, y poner todo en minúscula:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HHcMm72RHyUp"
      },
      "outputs": [],
      "source": [
        "# Cargamos la librerías a utilizar para normalizar el texto en español\n",
        "#import unidecode\n",
        "\n",
        "#Definimos nuestra función\n",
        "def limpiadora(x):\n",
        "    if isinstance(x, list):\n",
        "        #para listas\n",
        "        return [unidecode.unidecode(str.lower(i.replace(\" \", \"\"))) for i in x]\n",
        "    else:\n",
        "        #para strings\n",
        "        if isinstance(x, str):\n",
        "            return unidecode.unidecode(str.lower(x.replace(\" \", \"\")))\n",
        "        else:\n",
        "            return ''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4BoTKWvfHyUq"
      },
      "outputs": [],
      "source": [
        "pelis['genero']=pelis['genero'].apply(limpiadora)\n",
        "#Retenemos sólo los primeros tres generos mencionados\n",
        "pelis['genero'] =pelis['genero'].apply(lambda x: x[:3])\n",
        "pelis[['titulo','genero']].head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QnYiP07rHyUq"
      },
      "source": [
        "tenemos entonces \"limpia\" la columna de género. Procedemos de manera similar con el reparto, pero además en este ejercicio sólo nos quedaremos con los 4 actores principales"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KBRaVLPVHyUq"
      },
      "outputs": [],
      "source": [
        "#Creamos una lista de actores delimitados por puntos\n",
        "pelis['reparto']=pelis['reparto'].str.split(\",\")\n",
        "#\"Limpiamos\" los datos del reparto\n",
        "pelis['reparto']=pelis['reparto'].apply(limpiadora)\n",
        "#Retenemos sólo los primeros cuatro actores mencionados\n",
        "pelis['reparto'] =pelis['reparto'].apply(lambda x: x[:4])\n",
        "pelis[['titulo','genero','reparto','direccion']].head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V1aa4ldXHyUq"
      },
      "source": [
        "Finalmente nos queda la función de dirección. Notemos que aparece entre paréntesis la palabra creador al lado del director que también fue el creador. Utilizaremos entonces primero expresiones regulares para quitar esta palabra con los paréntesis y luego procederemos de manera similar a como lo hicimos anteriormente. Respecto al director solamente nos quedaremos con el individuo que aparece primero."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "txlIAabnHyUq"
      },
      "outputs": [],
      "source": [
        "#Removemos \"(Creador)\"\n",
        "pelis['direccion']=pelis['direccion'].str.replace(\"\\\\(Creador\\\\)\",\"\",regex=True)\n",
        "#Creamos una lista de actores delimitados por puntos\n",
        "pelis['direccion']=pelis['direccion'].str.split(\",\")\n",
        "#\"Limpiamos\" los datos de direccion\n",
        "pelis['direccion']=pelis['direccion'].apply(limpiadora)\n",
        "#Retenemos sólo el primer director\n",
        "pelis['direccion'] =pelis['direccion'].apply(lambda x: x[:1])\n",
        "pelis[['titulo','direccion']].head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZuuG34BhHyUr"
      },
      "source": [
        "Con estos datos entonces estamos en condiciones de generar una columna que combine toda esta información. Para ello crearemos una función que concatene estos caracteres:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UYtAFAdlHyUr"
      },
      "outputs": [],
      "source": [
        "def crear_metadatos(x):\n",
        "        return ' '.join(x['genero']) + ' ' + ' '.join(x['reparto']) + ' ' + ''.join(x['direccion'] )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nM1nUNDbHyUr"
      },
      "source": [
        "Aplicamos esta función a todas las filas creando la columna `metadato`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OAPZoc4THyUr"
      },
      "outputs": [],
      "source": [
        "pelis['metadatos'] = pelis.apply(crear_metadatos, axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "En6CcIsnHyUr"
      },
      "source": [
        "Veamos como luce la primer entrada:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NyBdwA_HHyUr"
      },
      "outputs": [],
      "source": [
        "pelis.iloc[0]['metadatos']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wdD4py-YHyUs"
      },
      "source": [
        "Tenemos entonces un vector que combina toda la información de las columnas: Chapelwaite es una serie de tv, drama, terror e intriga con las actuaciones de Adrien Brody, Jennifer Ens, Sirena Gulamgaus y Emily Hampshire, dirigida por Stephen King.\n",
        "\n",
        "### Generando recomendaciones\n",
        "\n",
        "Procederemos entonces de manera similar a la sección anterior. A diferencia de la sección previa utilizaremos `CountVectorizer` y no `TF-IDFVectorizer`. Esto se debe a que `TF-IDFVectorizer` penaliza a aquellos actores y directores que aparecen más frecuentemente, y esto potencialmente sesgaría las recomendaciones. Sin embargo, te invito a que pruebes como cambiarían las recomendaciones utilizando el `TF-IDFVectorizer`.\n",
        "\n",
        "Comenzamos importando  el objeto [CountVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) de [scikit](https://scikit-learn.org/stable/) que nos va a permitir crear la matriz con los documentos vectorizados, y utilizamos las *stopwords* definidas anteriormente"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Po-khufsHyUs"
      },
      "outputs": [],
      "source": [
        "# Import CountVectorizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "#Definimos un objeto CountVectorizer y creamos los vectores\n",
        "count = CountVectorizer(stop_words= list(lista_stopwords))\n",
        "count_matrix = count.fit_transform(pelis['metadatos'])\n",
        "count_matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_epIInFoHyUs"
      },
      "source": [
        "Generamos entonces una matriz poco densa de dimensión 1321 x 4849. Necesitamos entonces ahora calcular la similitud de coseno entre estos vectores. Para ello usamos la función [cosine_similarity](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.cosine_similarity.html)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uISDTh2FHyUs"
      },
      "outputs": [],
      "source": [
        "#Importamos la función\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "#Calculamos la matriz de similitud de coseno\n",
        "cosine_sim2 = cosine_similarity(count_matrix, count_matrix)\n",
        "\n",
        "cosine_sim2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ObcpURjKHyUt"
      },
      "source": [
        "Vemos entonces que la diagonal principal tendrá 1 ya que la similitud de cada película/serie consigo misma es uno.\n",
        "\n",
        "Con esta información el paso final es remplazar en la función que creamos en la sección anterior la matriz de similitud de coseno."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VApQ2x_qHyUt"
      },
      "outputs": [],
      "source": [
        "recom=recomendador('Las iguanas', cosine_sim=cosine_sim2, df=pelis)\n",
        "recom"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_X3ZOHIzHyUt"
      },
      "outputs": [],
      "source": [
        "# Cargamos las librerías a utilizar\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#creamoes un subset de las películas/series recomendadas\n",
        "pelis_rec=pelis[pelis['titulo'].isin(recom)]\n",
        "\n",
        "# Armamos la nube de palabras\n",
        "text = \" \".join(pelis_rec['metadatos'])\n",
        "wordcloud = WordCloud(width = 1600, height = 800, stopwords = set(lista_stopwords),\n",
        "    background_color = \"white\").generate(text)\n",
        "plt.figure(figsize = (20, 10))\n",
        "plt.imshow(wordcloud, interpolation = 'bilinear')\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sgiDFTklHyUt"
      },
      "source": [
        "Notemos que la recomendaciones son claramente distintas y se enfocaron en dramas que son series de televisión, y los actores aparecen más representados."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NvakLtMdHyUt"
      },
      "source": [
        "## Consideraciones finales\n",
        "\n",
        "Los recomendadores basados en contenido que creamos en este *cuaderno*, por supuesto, no se acercan a los poderosos modelos utilizados en la industria. Todavía existe mucho margen de mejora. Entre las mejoras que podemos pensar están:\n",
        "\n",
        "- Experimentar con la cantidad de géneros, actores y directores.\n",
        "- Crear subgéneros a partir de géneros, por ejemplo, comedia es el género y el comedia-negra el subgénero.\n",
        "- Dar más peso a ciertas características. Por ejemplo, podríamos darle más importancia al director repitiendo las veces que aparece en los metadatos.\n",
        "- Considerando otra información como ser miembros del equipo, premios, etc.\n",
        "- Combinando información de la trama con los metadatos y popularidad\n",
        "\n",
        "Te invito a que pruebes estas alternativas."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sH9mddoHHyUu"
      },
      "source": [
        "## Referencias\n",
        "\n",
        "- Banik, R. (2018). Hands-on recommendation systems with Python: start building powerful and personalized, recommendation engines with Python. Packt Publishing Ltd.\n",
        "\n",
        "- Fradejas Rueda, J. M. (2020). Cuentapalabras. Estilometrıa y análisis de texto con R para filólogos.\n",
        "\n",
        "- Google developers. (n.d.). Recommendation systems. Google. Consultado en Abril 3, 2022, de https://developers.google.com/machine-learning/recommendation/overview\n",
        "\n",
        "- Patel, A. A. (2019). Hands-on unsupervised learning using Python: how to build applied machine learning solutions from unlabeled data. O'Reilly Media.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "interpreter": {
      "hash": "fe36d3cf18f454bb22b210d1ce52ae8c21a1b2f0a9257a143474ae90bef14b60"
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}